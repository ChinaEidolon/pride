{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install pandas torch pytorch_transformers transformers scikit-learn nltk accelerate --upgrade\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from transformers import RobertaTokenizer, RobertaModel, Trainer, TrainingArguments\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# clean of special characters\n",
    "# lowercase it all\n",
    "# remove special character\n",
    "# then stem the words\n",
    "# finally tokenize the text\n",
    "\n",
    "\n",
    "# Load data\n",
    "friendspersona_full = pd.read_csv('friends-personality.csv')\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(X_text):\n",
    "    clean = re.sub(r'<.*?>', '', X_text)  # Remove HTML tags\n",
    "    clean = re.sub(r'\\s+', ' ', clean)  # Replace multiple spaces with a single space\n",
    "    return clean.strip()\n",
    "\n",
    "friendspersona_full[\"text_lower\"] = friendspersona_full[\"text\"].str.lower()\n",
    "\n",
    "replaceBArrows = []\n",
    "for sentence in friendspersona_full[\"text_lower\"]:\n",
    "    sentenceNew = sentence.replace('<b>', '').replace('</b>', '').replace('<br>', '')\n",
    "    replaceBArrows.append(sentenceNew)\n",
    "\n",
    "friendspersona_full[\"text_lower\"] = replaceBArrows\n",
    "\n",
    "# Remove special characters\n",
    "no_special_char = [re.sub('[^A-Za-z0-9]+', ' ', sentence) for sentence in friendspersona_full[\"text_lower\"]]\n",
    "friendspersona_full[\"no_special_char\"] = no_special_char\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "friendspersona_full[\"text_stemmed\"] = friendspersona_full[\"no_special_char\"].apply(lambda text: stem_words(text))\n",
    "\n",
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "def tokenize_text(text, max_length=512):\n",
    "    return tokenizer(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "friendspersona_full['clean_text'] = friendspersona_full['text_stemmed'].apply(clean_text)\n",
    "friendspersona_full['tokenized_text'] = friendspersona_full['clean_text'].apply(lambda x: tokenize_text(x))\n",
    "\n",
    "X_text = friendspersona_full['tokenized_text'].tolist()\n",
    "print(f\"X_text sample: {X_text[:5]}\")\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "X_character = onehot_encoder.fit_transform(friendspersona_full[['character']]).toarray()\n",
    "\n",
    "y = friendspersona_full[['cAGR', 'cCON', 'cEXT', 'cOPN', 'cNEU']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(X_text) == len(X_character) == len(y), \"Mismatch in data lengths\"\n",
    "\n",
    "# Train-test split\n",
    "X_text_train, X_text_test, X_character_train, X_character_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_character, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "class PersonalityDataset(Dataset):\n",
    "    def __init__(self, text_data, character_data, labels):\n",
    "        self.text_data = text_data\n",
    "        self.character_data = character_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        item = {\n",
    "            'input_ids': text['input_ids'].squeeze(0).to(device),\n",
    "            'attention_mask': text['attention_mask'].squeeze(0).to(device),\n",
    "            'character_input': torch.tensor(self.character_data[idx], dtype=torch.float).to(device),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float).to(device),\n",
    "        }\n",
    "        return item\n",
    "    \n",
    "    \n",
    "# Create datasets\n",
    "train_dataset = PersonalityDataset(X_text_train, X_character_train, y_train)\n",
    "test_dataset = PersonalityDataset(X_text_test, X_character_test, y_test)\n",
    "\n",
    "# Print a sample from the dataset\n",
    "print(\"Sample from train_dataset:\", train_dataset[0])\n",
    "print(\"Sample from test_dataset:\", test_dataset[0])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class CustomRobertaModel(nn.Module):\n",
    "    def __init__(self, num_labels=5, character_input_dim=106):\n",
    "        super(CustomRobertaModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.character_dense = nn.Linear(character_input_dim, 128)\n",
    "        self.dense_1 = nn.Linear(768 + 128, 256)\n",
    "        self.dropout_1 = nn.Dropout(0.5)\n",
    "        self.dense_2 = nn.Linear(256, 128)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(128, 5)\n",
    "        self.loss_fn = nn.MSELoss()  # Add this line\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, character_input, attention_mask, labels = None):\n",
    "        roberta_output = self.roberta(input_ids, attention_mask=attention_mask)[1]\n",
    "        character_output = torch.relu(self.character_dense(character_input))\n",
    "        combined = torch.cat((roberta_output, character_output), dim=1)\n",
    "        x = torch.relu(self.dense_1(combined))\n",
    "        x = self.dropout_1(x)\n",
    "        x = torch.relu(self.dense_2(x))\n",
    "        x = self.dropout_2(x)\n",
    "        logits = self.output(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "# Initialize the model\n",
    "roberta_model = CustomRobertaModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator function\n",
    "\n",
    "# all this does is pad the data, and split it into batches. batches is the \n",
    "# input_ids is the text_data\n",
    "\n",
    "# i think we ran into issues when we tried \n",
    "\n",
    "# Define the data collator function\n",
    "# inside this data collator features parameter, contains the features, and it has\n",
    "# a dictionary with these 4 values\n",
    "\n",
    "#where is the dictionary defined?           \n",
    "# To debug this issue, we need to ensure that the dataset correctly returns dictionaries  \n",
    "# with the expected keys ('input_ids', 'character_input', 'attention_mask', and 'labels').\n",
    "# Let's go through the process step-by-step. \n",
    "\n",
    "# if dictionary is defined in __getitem__, i have a hunch that we don't actually\n",
    "# use the __getitem__ function for some reason\n",
    "\n",
    "# okay so there's no issue with the __getitem___ function...\n",
    "# but there is a problem with what we pass into features\n",
    "\n",
    "\n",
    "\n",
    "# apparently features is a list of dictionaries, and each dictionary correspondes to a \n",
    "# sample from dataset, returned by __getitem__\n",
    "\n",
    "\n",
    "# explanation: train dataset has 3 sample datapoints\n",
    "# dataloader splits the train dataset into batches of size 2\n",
    "# dataloader sends a batch over to data_collator in the form of features parameter\n",
    "\n",
    "def data_collator(features):\n",
    "    for i, feature in enumerate(features):\n",
    "        if 'input_ids' not in feature:\n",
    "            print(f\"Missing 'input_ids' in feature {i}: {feature}\")\n",
    "        if 'attention_mask' not in feature:\n",
    "            print(f\"Missing 'attention_mask' in feature {i}: {feature}\")\n",
    "        if 'character_input' not in feature:\n",
    "            print(f\"Missing 'character_input' in feature {i}: {feature}\")\n",
    "        if 'labels' not in feature:\n",
    "            print(f\"Missing 'labels' in feature {i}: {feature}\")\n",
    "\n",
    "    input_ids = [f['input_ids'] for f in features]\n",
    "    attention_mask = [f['attention_mask'] for f in features]\n",
    "    character_input = [f['character_input'] for f in features]\n",
    "    labels = [f['labels'] for f in features]\n",
    "\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    character_input = torch.stack(character_input).to(device)\n",
    "    labels = torch.stack(labels).to(device)\n",
    "\n",
    "    batched_data = {\n",
    "        'input_ids': padded_input_ids,\n",
    "        'character_input': character_input,\n",
    "        'attention_mask': padded_attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "\n",
    "# When the model receives inputs that include the labels, it's supposed to produce a \n",
    "# tuple of (loss, predictions), where the loss is a scalar. The trainer then uses the l\n",
    "# oss to calculate the gradients. In this case (or at least in my case when I get a \n",
    "# similar error) the trainer appears to be trying to use the predictions not the\n",
    "# loss to calculate the gradient. This appears to be because the model is not receiving \n",
    "# the 'labels' as input and so is only producing a one tuple of (predictions).\n",
    "# You should be able to fix it by passing a value for \"labels\" in your collator. \n",
    "# See for example transformers.DataCollatorForLanguageModeling.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    # remove_unused_columns=False,\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    "    # device=device  # Add this line\n",
    ")\n",
    "# i will change compute_metrics later if I want to track specific metrics\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# okay so the problem is definitely 100% what dataset we are passing into data_collator\n",
    "# it is missing some keys in the dictionary and I have no idea why\n",
    "# hypothesis: some features are missing keys, some aren't?\n",
    "# add a console.err debug that outputs the current features.\n",
    "# or just output the current features everytime\n",
    "# apparently [0] already has problems\n",
    "\n",
    "# from transformers import DataCollatorForTokenClassification\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# ohhh, features from data_collator is a batch!\n",
    "\n",
    "\n",
    "# # Print sample from the dataset to debug\n",
    "# sample = train_dataset[0]\n",
    "# print(\"Sample from dataset:\", sample)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# either it is a problem with grad can be implicitly created https://github.com/huggingface/transformers/issues/6749\n",
    "# or it is an issue with keeping labels in the datakey dictionary, so we should just remove the labels key because it's handled separately???? in train????\n",
    "    # tried just removing the labels key everywhere, doesn't work\n",
    "\n",
    "# need to figure this part out\n",
    "# apparently it's because of the model. Bascially, my loss caluclation needs to be in my model\n",
    "# it's not returning a loss value, necessary for Trainer to compute gradients\n",
    "\n",
    "# you need to return a loss value????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"roberta_model.pt\"\n",
    "\n",
    "# Save\n",
    "trainer.save_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install safetensors\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"roberta_model\"  # This is the directory containing your saved model\n",
    "\n",
    "state_dict = load_file(f\"{PATH}/model.safetensors\")\n",
    "\n",
    "roberta_model.load_state_dict(state_dict)\n",
    "\n",
    "roberta_model.eval()\n",
    "roberta_model.to(device)\n",
    "\n",
    "\n",
    "# Load training arguments\n",
    "training_args = torch.load(f\"{PATH}/training_args.bin\")\n",
    "\n",
    "# Recreate the Trainer with the loaded model\n",
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # Use the same data_collator you used for training\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character_encoded = onehot_encoder.transform([[\"okay\"]]).toarray()\n",
    "\n",
    "# # [0. 0. 0. 0. 0. 0.]\n",
    "\n",
    "def prepare_input(text, character):\n",
    "    # Clean and tokenize the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    no_special_char = [re.sub('[^A-Za-z0-9]+', ' ', cleaned_text)]\n",
    "    no_marks = no_special_char[0].replace('<b>', '').replace('</b>', '').replace('<br>', '')\n",
    "    print(no_marks)\n",
    "    tokenized = tokenize_text(no_marks)\n",
    "    print(\"tokenized\")\n",
    "    print(tokenized)\n",
    "    # One-hot encode the character\n",
    "    character_encoded = onehot_encoder.transform([[character]]).toarray()\n",
    "    \n",
    "    # Create a dataset with a single item\n",
    "    dataset = PersonalityDataset([tokenized], [character_encoded[0]], [[0, 0, 0, 0, 0]])  # Dummy labels\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def predict(trainer, dataset):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    return predictions.predictions[0]  # Return the first (and only) prediction\n",
    "\n",
    "# Example usage\n",
    "new_text = \"<b>s01_e01_c01(1) for Joey Tribbiani</b><br><br><b>Ross Geller</b>: No!! Okay?! Why does everyone keep fixating on that? She didn't know, how should I know?<br><br><b>Chandler Bing</b>: Sometimes I wish I was a lesbian... (They all stare at him.) Did I say that out loud?<br><br><b>Ross Geller</b>: I told mom and dad last night, they seemed to take it pretty well.<br><br><b>Monica Geller</b>: Oh really, so that hysterical phone call I got from a woman at sobbing 3:00 A.M., 'I'll never have grandchildren, I'll never have grandchildren.' was what? A wrong number?<br><br><b>Ross Geller</b>: Sorry.<br><br><b>Joey Tribbiani</b>: Alright Ross, look. You're feeling a lot of pain right now. You're angry. You're hurting. Can I tell you what the answer is?<br><br>(Ross gestures his consent.)<br><br><b>Joey Tribbiani</b>: Strip joint! C'mon, you're single! Have some hormones!<br><br><b>Ross Geller</b>: I don't want to be single, okay? I just... I just- I just wanna be married again!<br><br>(Rachel enters in a wet wedding dress and starts to search the room.)<br><br>\"\n",
    "new_character = \"Joey Tribbiani\"\n",
    "prediction_dataset = prepare_input(new_text, new_character)\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "predictions = predict(trainer, prediction_dataset)\n",
    "\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\")\n",
    "print(\"cAGR:\", predictions[0])\n",
    "print(\"cCON:\", predictions[1])\n",
    "print(\"cEXT:\", predictions[2])\n",
    "print(\"cOPN:\", predictions[3])\n",
    "print(\"cNEU:\", predictions[4])\n",
    "\n",
    "# the real output of [0] is supposed to be 1,1,0,0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(friendspersona_full['tokenized_text'][0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
