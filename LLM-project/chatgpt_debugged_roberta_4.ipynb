{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.1-cp39-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting pytorch_transformers\n",
      "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-2.0.0-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/boqiangliang/Documents/pride/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/boqiangliang/Documents/pride/.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting boto3 (from pytorch_transformers)\n",
      "  Downloading boto3-1.34.138-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting requests (from pytorch_transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from pytorch_transformers)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex (from pytorch_transformers)\n",
      "  Downloading regex-2024.5.15-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from pytorch_transformers)\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting sacremoses (from pytorch_transformers)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/boqiangliang/Documents/pride/.venv/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: psutil in /Users/boqiangliang/Documents/pride/.venv/lib/python3.9/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/boqiangliang/Documents/pride/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting botocore<1.35.0,>=1.34.138 (from boto3->pytorch_transformers)\n",
      "  Downloading botocore-1.34.138-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_transformers)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_transformers)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->pytorch_transformers)\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->pytorch_transformers)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->pytorch_transformers)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->pytorch_transformers)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->pytorch_transformers)\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.1-cp39-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/61.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:29\u001b[0mm^C\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/61.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:29\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m python3 -m pip install pandas torch pytorch_transformers transformers scikit-learn nltk accelerate --upgrade\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install pandas torch pytorch_transformers transformers scikit-learn nltk accelerate --upgrade\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "from transformers import RobertaTokenizer, RobertaModel, Trainer, TrainingArguments\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# clean of special characters\n",
    "# lowercase it all\n",
    "# remove special character\n",
    "# then stem the words\n",
    "# finally tokenize the text\n",
    "\n",
    "\n",
    "# Load data\n",
    "friendspersona_full = pd.read_csv('friends-personality.csv')\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(X_text):\n",
    "    clean = re.sub(r'<.*?>', '', X_text)  # Remove HTML tags\n",
    "    clean = re.sub(r'\\s+', ' ', clean)  # Replace multiple spaces with a single space\n",
    "    return clean.strip()\n",
    "\n",
    "friendspersona_full[\"text_lower\"] = friendspersona_full[\"text\"].str.lower()\n",
    "\n",
    "replaceBArrows = []\n",
    "for sentence in friendspersona_full[\"text_lower\"]:\n",
    "    sentenceNew = sentence.replace('<b>', '').replace('</b>', '').replace('<br>', '')\n",
    "    replaceBArrows.append(sentenceNew)\n",
    "\n",
    "friendspersona_full[\"text_lower\"] = replaceBArrows\n",
    "\n",
    "# Remove special characters\n",
    "no_special_char = [re.sub('[^A-Za-z0-9]+', ' ', sentence) for sentence in friendspersona_full[\"text_lower\"]]\n",
    "friendspersona_full[\"no_special_char\"] = no_special_char\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "friendspersona_full[\"text_stemmed\"] = friendspersona_full[\"no_special_char\"].apply(lambda text: stem_words(text))\n",
    "\n",
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "def tokenize_text(text, max_length=512):\n",
    "    return tokenizer(text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "\n",
    "friendspersona_full['clean_text'] = friendspersona_full['text_stemmed'].apply(clean_text)\n",
    "friendspersona_full['tokenized_text'] = friendspersona_full['clean_text'].apply(lambda x: tokenize_text(x))\n",
    "\n",
    "X_text = friendspersona_full['tokenized_text'].tolist()\n",
    "print(f\"X_text sample: {X_text[:5]}\")\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "X_character = onehot_encoder.fit_transform(friendspersona_full[['character']]).toarray()\n",
    "\n",
    "y = friendspersona_full[['cAGR', 'cCON', 'cEXT', 'cOPN', 'cNEU']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(X_text) == len(X_character) == len(y), \"Mismatch in data lengths\"\n",
    "\n",
    "# Train-test split\n",
    "X_text_train, X_text_test, X_character_train, X_character_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_character, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "class PersonalityDataset(Dataset):\n",
    "    def __init__(self, text_data, character_data, labels):\n",
    "        self.text_data = text_data\n",
    "        self.character_data = character_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_data[idx]\n",
    "        item = {\n",
    "            'input_ids': text['input_ids'].squeeze(0).to(device),\n",
    "            'attention_mask': text['attention_mask'].squeeze(0).to(device),\n",
    "            'character_input': torch.tensor(self.character_data[idx], dtype=torch.float).to(device),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float).to(device),\n",
    "        }\n",
    "        return item\n",
    "    \n",
    "    \n",
    "# Create datasets\n",
    "train_dataset = PersonalityDataset(X_text_train, X_character_train, y_train)\n",
    "test_dataset = PersonalityDataset(X_text_test, X_character_test, y_test)\n",
    "\n",
    "# Print a sample from the dataset\n",
    "print(\"Sample from train_dataset:\", train_dataset[0])\n",
    "print(\"Sample from test_dataset:\", test_dataset[0])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class CustomRobertaModel(nn.Module):\n",
    "    def __init__(self, num_labels=5, character_input_dim=106):\n",
    "        super(CustomRobertaModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.character_dense = nn.Linear(character_input_dim, 128)\n",
    "        self.dense_1 = nn.Linear(768 + 128, 256)\n",
    "        self.dropout_1 = nn.Dropout(0.5)\n",
    "        self.dense_2 = nn.Linear(256, 128)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(128, 5)\n",
    "        self.loss_fn = nn.MSELoss()  # Add this line\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, character_input, attention_mask, labels = None):\n",
    "        roberta_output = self.roberta(input_ids, attention_mask=attention_mask)[1]\n",
    "        character_output = torch.relu(self.character_dense(character_input))\n",
    "        combined = torch.cat((roberta_output, character_output), dim=1)\n",
    "        x = torch.relu(self.dense_1(combined))\n",
    "        x = self.dropout_1(x)\n",
    "        x = torch.relu(self.dense_2(x))\n",
    "        x = self.dropout_2(x)\n",
    "        logits = self.output(x)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "# Initialize the model\n",
    "roberta_model = CustomRobertaModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator function\n",
    "\n",
    "# all this does is pad the data, and split it into batches. batches is the \n",
    "# input_ids is the text_data\n",
    "\n",
    "# i think we ran into issues when we tried \n",
    "\n",
    "# Define the data collator function\n",
    "# inside this data collator features parameter, contains the features, and it has\n",
    "# a dictionary with these 4 values\n",
    "\n",
    "#where is the dictionary defined?           \n",
    "# To debug this issue, we need to ensure that the dataset correctly returns dictionaries  \n",
    "# with the expected keys ('input_ids', 'character_input', 'attention_mask', and 'labels').\n",
    "# Let's go through the process step-by-step. \n",
    "\n",
    "# if dictionary is defined in __getitem__, i have a hunch that we don't actually\n",
    "# use the __getitem__ function for some reason\n",
    "\n",
    "# okay so there's no issue with the __getitem___ function...\n",
    "# but there is a problem with what we pass into features\n",
    "\n",
    "\n",
    "\n",
    "# apparently features is a list of dictionaries, and each dictionary correspondes to a \n",
    "# sample from dataset, returned by __getitem__\n",
    "\n",
    "\n",
    "# explanation: train dataset has 3 sample datapoints\n",
    "# dataloader splits the train dataset into batches of size 2\n",
    "# dataloader sends a batch over to data_collator in the form of features parameter\n",
    "\n",
    "def data_collator(features):\n",
    "    for i, feature in enumerate(features):\n",
    "        if 'input_ids' not in feature:\n",
    "            print(f\"Missing 'input_ids' in feature {i}: {feature}\")\n",
    "        if 'attention_mask' not in feature:\n",
    "            print(f\"Missing 'attention_mask' in feature {i}: {feature}\")\n",
    "        if 'character_input' not in feature:\n",
    "            print(f\"Missing 'character_input' in feature {i}: {feature}\")\n",
    "        if 'labels' not in feature:\n",
    "            print(f\"Missing 'labels' in feature {i}: {feature}\")\n",
    "\n",
    "    input_ids = [f['input_ids'] for f in features]\n",
    "    attention_mask = [f['attention_mask'] for f in features]\n",
    "    character_input = [f['character_input'] for f in features]\n",
    "    labels = [f['labels'] for f in features]\n",
    "\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "    padded_attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0).to(device)\n",
    "\n",
    "    character_input = torch.stack(character_input).to(device)\n",
    "    labels = torch.stack(labels).to(device)\n",
    "\n",
    "    batched_data = {\n",
    "        'input_ids': padded_input_ids,\n",
    "        'character_input': character_input,\n",
    "        'attention_mask': padded_attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "\n",
    "# When the model receives inputs that include the labels, it's supposed to produce a \n",
    "# tuple of (loss, predictions), where the loss is a scalar. The trainer then uses the l\n",
    "# oss to calculate the gradients. In this case (or at least in my case when I get a \n",
    "# similar error) the trainer appears to be trying to use the predictions not the\n",
    "# loss to calculate the gradient. This appears to be because the model is not receiving \n",
    "# the 'labels' as input and so is only producing a one tuple of (predictions).\n",
    "# You should be able to fix it by passing a value for \"labels\" in your collator. \n",
    "# See for example transformers.DataCollatorForLanguageModeling.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    # remove_unused_columns=False,\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    "    # device=device  # Add this line\n",
    ")\n",
    "# i will change compute_metrics later if I want to track specific metrics\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "# okay so the problem is definitely 100% what dataset we are passing into data_collator\n",
    "# it is missing some keys in the dictionary and I have no idea why\n",
    "# hypothesis: some features are missing keys, some aren't?\n",
    "# add a console.err debug that outputs the current features.\n",
    "# or just output the current features everytime\n",
    "# apparently [0] already has problems\n",
    "\n",
    "# from transformers import DataCollatorForTokenClassification\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# ohhh, features from data_collator is a batch!\n",
    "\n",
    "\n",
    "# # Print sample from the dataset to debug\n",
    "# sample = train_dataset[0]\n",
    "# print(\"Sample from dataset:\", sample)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# either it is a problem with grad can be implicitly created https://github.com/huggingface/transformers/issues/6749\n",
    "# or it is an issue with keeping labels in the datakey dictionary, so we should just remove the labels key because it's handled separately???? in train????\n",
    "    # tried just removing the labels key everywhere, doesn't work\n",
    "\n",
    "# need to figure this part out\n",
    "# apparently it's because of the model. Bascially, my loss caluclation needs to be in my model\n",
    "# it's not returning a loss value, necessary for Trainer to compute gradients\n",
    "\n",
    "# you need to return a loss value????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"roberta_model.pt\"\n",
    "\n",
    "# Save\n",
    "trainer.save_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install safetensors\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"roberta_model\"  # This is the directory containing your saved model\n",
    "\n",
    "state_dict = load_file(f\"{PATH}/model.safetensors\")\n",
    "\n",
    "roberta_model.load_state_dict(state_dict)\n",
    "\n",
    "roberta_model.eval()\n",
    "roberta_model.to(device)\n",
    "\n",
    "\n",
    "# Load training arguments\n",
    "training_args = torch.load(f\"{PATH}/training_args.bin\")\n",
    "\n",
    "# Recreate the Trainer with the loaded model\n",
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # Use the same data_collator you used for training\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character_encoded = onehot_encoder.transform([[\"okay\"]]).toarray()\n",
    "\n",
    "# # [0. 0. 0. 0. 0. 0.]\n",
    "\n",
    "def prepare_input(text, character):\n",
    "    # Clean and tokenize the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    no_special_char = [re.sub('[^A-Za-z0-9]+', ' ', cleaned_text)]\n",
    "    no_marks = no_special_char[0].replace('<b>', '').replace('</b>', '').replace('<br>', '')\n",
    "    print(no_marks)\n",
    "    tokenized = tokenize_text(no_marks)\n",
    "    print(\"tokenized\")\n",
    "    print(tokenized)\n",
    "    # One-hot encode the character\n",
    "    character_encoded = onehot_encoder.transform([[character]]).toarray()\n",
    "    \n",
    "    # Create a dataset with a single item\n",
    "    dataset = PersonalityDataset([tokenized], [character_encoded[0]], [[0, 0, 0, 0, 0]])  # Dummy labels\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def predict(trainer, dataset):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    return predictions.predictions[0]  # Return the first (and only) prediction\n",
    "\n",
    "# Example usage\n",
    "new_text = \"<b>s01_e01_c01(1) for Joey Tribbiani</b><br><br><b>Ross Geller</b>: No!! Okay?! Why does everyone keep fixating on that? She didn't know, how should I know?<br><br><b>Chandler Bing</b>: Sometimes I wish I was a lesbian... (They all stare at him.) Did I say that out loud?<br><br><b>Ross Geller</b>: I told mom and dad last night, they seemed to take it pretty well.<br><br><b>Monica Geller</b>: Oh really, so that hysterical phone call I got from a woman at sobbing 3:00 A.M., 'I'll never have grandchildren, I'll never have grandchildren.' was what? A wrong number?<br><br><b>Ross Geller</b>: Sorry.<br><br><b>Joey Tribbiani</b>: Alright Ross, look. You're feeling a lot of pain right now. You're angry. You're hurting. Can I tell you what the answer is?<br><br>(Ross gestures his consent.)<br><br><b>Joey Tribbiani</b>: Strip joint! C'mon, you're single! Have some hormones!<br><br><b>Ross Geller</b>: I don't want to be single, okay? I just... I just- I just wanna be married again!<br><br>(Rachel enters in a wet wedding dress and starts to search the room.)<br><br>\"\n",
    "new_character = \"Joey Tribbiani\"\n",
    "prediction_dataset = prepare_input(new_text, new_character)\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction\n",
    "predictions = predict(trainer, prediction_dataset)\n",
    "\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\")\n",
    "print(\"cAGR:\", predictions[0])\n",
    "print(\"cCON:\", predictions[1])\n",
    "print(\"cEXT:\", predictions[2])\n",
    "print(\"cOPN:\", predictions[3])\n",
    "print(\"cNEU:\", predictions[4])\n",
    "\n",
    "# the real output of [0] is supposed to be 1,1,0,0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(friendspersona_full['tokenized_text'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
